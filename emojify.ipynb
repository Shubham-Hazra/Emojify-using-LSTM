{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The emoji dictionary\n",
    "emoji_dictionary = {#\"0\": \":red_heart:\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"0\": \"\\u2764\\ufe0f\",\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv('emojify_data.csv')\n",
    "X_train = np.array(train.iloc[:, 0])\n",
    "Y_train = np.array(train.iloc[:, 1], dtype=int)\n",
    "test = pd.read_csv('tesss.csv')\n",
    "X_test = np.array(test.iloc[:, 0])\n",
    "Y_test = np.array(test.iloc[:, 1], dtype=int)\n",
    "y_train = tf.keras.utils.to_categorical(Y_train, num_classes=5)\n",
    "y_test = tf.keras.utils.to_categorical(Y_test, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of a sentence\n",
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the emoji for a label\n",
    "def label_to_emoji(label):\n",
    "    return emoji.emojize(emoji_dictionary[str(label)],language='alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work is horrible 😞\n",
      "I am upset 😞\n",
      "throw the ball ⚾\n",
      "Good joke 😄\n",
      "what is your favorite baseball game ⚾\n",
      "I cooked meat 🍴\n",
      "stop messing around 😞\n",
      "I want chinese food 🍴\n",
      "Let us go play baseball ⚾\n",
      "you are failing this exercise 😞\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 examples\n",
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained GloVe vectors\n",
    "word_to_index = {}\n",
    "words = []\n",
    "embeddings = {}\n",
    "index = 0\n",
    "with open('glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        words.append(word)\n",
    "        word_to_index[word] = index\n",
    "        embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the average of the word embeddings\n",
    "def sentence_to_avg(sentence,embeddings):\n",
    "    words = sentence.lower().split()\n",
    "    word_in_embedding = list(embeddings.keys())[0]\n",
    "    avg = np.zeros((embeddings[word_in_embedding].shape[0]))\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in embeddings:\n",
    "            avg += embeddings[w]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(array):\n",
    "    exps = np.exp(array)\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the emoji for a batch of sentences\n",
    "def predict(X, Y, W, b, embeddings):\n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    any_word = list(embeddings.keys())[0]  \n",
    "    n_h = embeddings[any_word].shape[0] \n",
    "    for j in range(m):                     \n",
    "        words = X[j].lower().split()\n",
    "        avg = np.zeros((n_h,))\n",
    "        count = 0\n",
    "        for w in words:\n",
    "            if w in embeddings:\n",
    "                avg += embeddings[w]\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            avg = avg / count\n",
    "        Z = np.dot(W, avg) + b\n",
    "        A = softmax(Z)\n",
    "        pred[j] = np.argmax(A)       \n",
    "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the model\n",
    "def model(X, Y, embeddings, learning_rate = 0.01, num_iterations = 200):\n",
    "    word = list(embeddings.keys())[0]\n",
    "    embedding_size = embeddings[word].shape[0]\n",
    "    m = X.shape[0]\n",
    "    n_y = len(np.unique(Y))\n",
    "    W = np.random.randn(n_y, embedding_size)\n",
    "    b = np.zeros((n_y,))\n",
    "    y = tf.keras.utils.to_categorical(Y, num_classes=n_y)\n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            avg = sentence_to_avg(X[i], embeddings)\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "            cost = -np.sum(y[i] * np.log(a))\n",
    "            dz = a - y[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1,embedding_size))\n",
    "            db = dz\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        if t % 10 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, embeddings)\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 2.0987714537945727\n",
      "Accuracy: 0.22527472527472528\n",
      "Epoch: 10 --- cost = 1.1391071163700783\n",
      "Accuracy: 0.489010989010989\n",
      "Epoch: 20 --- cost = 0.7575083029379175\n",
      "Accuracy: 0.6428571428571429\n",
      "Epoch: 30 --- cost = 0.5585546350928853\n",
      "Accuracy: 0.7417582417582418\n",
      "Epoch: 40 --- cost = 0.42666402753596616\n",
      "Accuracy: 0.8076923076923077\n",
      "Epoch: 50 --- cost = 0.3362730269638698\n",
      "Accuracy: 0.8461538461538461\n",
      "Epoch: 60 --- cost = 0.2726578037606863\n",
      "Accuracy: 0.8516483516483516\n",
      "Epoch: 70 --- cost = 0.22640001245872207\n",
      "Accuracy: 0.8736263736263736\n",
      "Epoch: 80 --- cost = 0.19169889600362527\n",
      "Accuracy: 0.8846153846153846\n",
      "Epoch: 90 --- cost = 0.16503920849618733\n",
      "Accuracy: 0.9010989010989011\n",
      "Epoch: 100 --- cost = 0.14414649144328942\n",
      "Accuracy: 0.9120879120879121\n",
      "Epoch: 110 --- cost = 0.127422394372808\n",
      "Accuracy: 0.9230769230769231\n",
      "Epoch: 120 --- cost = 0.1137393470556019\n",
      "Accuracy: 0.9230769230769231\n",
      "Epoch: 130 --- cost = 0.10231934328486482\n",
      "Accuracy: 0.9340659340659341\n",
      "Epoch: 140 --- cost = 0.09262710625411999\n",
      "Accuracy: 0.9395604395604396\n",
      "Epoch: 150 --- cost = 0.08428858117994849\n",
      "Accuracy: 0.945054945054945\n",
      "Epoch: 160 --- cost = 0.07703588668997223\n",
      "Accuracy: 0.945054945054945\n",
      "Epoch: 170 --- cost = 0.0706718659439709\n",
      "Accuracy: 0.9505494505494505\n",
      "Epoch: 180 --- cost = 0.06504745780493752\n",
      "Accuracy: 0.9505494505494505\n",
      "Epoch: 190 --- cost = 0.06004712572747786\n",
      "Accuracy: 0.9560439560439561\n",
      "[[3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "pred, W, b = model(X_train, Y_train, embeddings)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9560439560439561\n",
      "Test set:\n",
      "Accuracy: 0.8363636363636363\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, embeddings)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the emoji for a sentence\n",
    "def predict_single(sentence, W=W, b=b, embeddings=embeddings):\n",
    "    avg = sentence_to_avg(sentence, embeddings)\n",
    "    z = np.dot(W, avg) + b\n",
    "    a = softmax(z)\n",
    "    return np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🍴'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the emoji for a sentence\n",
    "label_to_emoji(int(predict_single(\"Lets eat\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of the words for a sentence\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            j = j + 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding layer\n",
    "def pretrained_embedding_layer(embeddings, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = len(embeddings[list(embeddings.keys())[0]])\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = embeddings[word]\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = tf.keras.layers.Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    X = tf.keras.layers.LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = tf.keras.layers.Dropout(0.5)(X)\n",
    "    X = tf.keras.layers.LSTM(128, return_sequences=False)(X)\n",
    "    X = tf.keras.layers.Dropout(0.5)(X)\n",
    "    X = tf.keras.layers.Dense(5)(X)\n",
    "    X = tf.keras.layers.Activation('softmax')(X)\n",
    "    model = tf.keras.models.Model(inputs=sentence_indices, outputs=X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 10, 100)           40000100  \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10, 128)           117248    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10, 128)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,249,577\n",
      "Trainable params: 249,477\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Emojify_V2((maxLen,), embeddings, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 33ms/step - loss: 1.6959e-05 - accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.1103e-05 - accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.9718e-05 - accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5798e-05 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6043e-05 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1985e-05 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6060e-05 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.5940e-05 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7238e-05 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.7387e-05 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2483e-05 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.0222e-05 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6245e-05 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4468e-05 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4241e-05 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0859e-05 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.0010e-05 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.5956e-05 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3093e-05 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2390e-05 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2964e-05 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2919e-05 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2960e-05 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4252e-05 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.0973e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5690e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.6606e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.3512e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.6516e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3181e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7974e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1145e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3339e-05 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.8294e-05 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9183e-05 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2979e-05 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4547e-05 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3420e-05 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7064e-05 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5770e-05 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0040e-05 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4897e-05 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1361e-05 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7376e-05 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2044e-05 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6617e-05 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8202e-05 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1927e-05 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.0970e-06 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0023e-05 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2032e-05 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0888e-05 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2864e-05 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1860e-05 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2298e-05 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5459e-05 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3423e-05 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8266e-06 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5550e-05 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.0404e-05 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8005e-05 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0692e-05 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2095e-05 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4443e-05 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.9770e-06 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0736e-05 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.2770e-06 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.9795e-05 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2580e-05 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0624e-05 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.2914e-05 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1033e-05 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0267e-05 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2674e-05 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4349e-05 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1714e-05 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4467e-05 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.6018e-06 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.5966e-06 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1576e-05 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1969e-05 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0146e-05 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2651e-05 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4185e-05 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.1047e-05 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.0389e-05 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.9180e-05 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.4687e-05 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.0955e-05 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.0780e-05 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 9.9862e-06 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1.0750e-05 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.2488e-05 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 2.2902e-05 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.4732e-05 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.5761e-05 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.7341e-05 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.1492e-05 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4078e-05 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.6027e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e897f6f040>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "model.fit(X_train_indices, y_train, epochs = 100, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 2.2061 - accuracy: 0.8364\n",
      "Test accuracy =  83.63636136054993\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, y_test)\n",
    "print(\"Test accuracy = \", acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "wow 😄\n"
     ]
    }
   ],
   "source": [
    "# Predict the emoji for a sentence\n",
    "x_test = np.array([\"wow\"])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ce76490c07a75ea24134e09186af2521c358252e3ea70d858a2691e4a58fda3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
